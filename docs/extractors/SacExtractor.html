<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>extractors.SacExtractor API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>extractors.SacExtractor</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">#
#
# from selenium import webdriver
# from selenium.webdriver.common.by import By
# from selenium.webdriver.common.keys import Keys
# import pandas as pd
#
#
# ###########################################################################
# # Set up Keyring in GitHub
# # - Ask for Username and Password if not already saved in the OS keyring
# # - Store the new Credentials in the Keystore
#
# import keyring
# import getpass
#
# keyringServiceNAme = &#34;SAC Weblogin&#34;
#
# while True:
#     credentials = keyring.get_credential(keyringServiceNAme, None)
#     if credentials == None:
#         try:
#             username = input(&#39;Please enter SAC Username: &#39;)
#         except Exception as error:
#             print(&#39;USERNAME READ ERROR&#39;, error)
#         try:
#             password = getpass.getpass(prompt=&#39;Please enter SAC Password: &#39;)
#         except Exception as error:
#             print(&#39;PASSWORD READ ERROR&#39;, error)
#         try:
#             keyring.set_password(keyringServiceNAme, username, password)
#         except Exception as error:
#             print(&#39;PASSWORD SAVE ERROR&#39;, error)
#     else: # we have a username and password
#         break
#
# #print(&#34;Scrapping with user: &#34; + credentials.username)
# #print(&#34;and password: &#34;+ credentials.password)
#
# ###########################################################################
#
# # Get to the target page sign in and filter# Set up webdriver
# print(&#34;start&#34;)
# driver = webdriver.Chrome()
#
# # Open URL
# url = &#39;https://www.sac-cas.ch/en/login/?redirect_url=%2Fen%2Fhuts-and-tours%2Fsac-route-portal%2F&amp;cHash=61fe243516fab7669ff192457a7ef6c5&#39;
# driver.get(url)
# driver.implicitly_wait(1)
#
# # Login
# driver.find_element(by=By.XPATH, value=&#39;//input[@name=&#34;username&#34;]&#39;).send_keys(credentials.username)
# driver.find_element(by=By.XPATH, value=&#39;//input[@name=&#34;password&#34;]&#39;).send_keys(credentials.password)
# driver.find_element(by=By.XPATH, value=&#39;//button[@type=&#34;submit&#34;]&#39;).click()
#
# # Dismiss Cookie
# driver.implicitly_wait(2)
# driver.find_element(by=By.XPATH, value=&#39;//button[@id=&#34;CybotCookiebotDialogBodyButtonDecline&#34;]&#39;).click()
#
# # Select mountain_hiking icon
# driver.implicitly_wait(5)
# driver.find_element(by=By.XPATH, value=&#39;//form[@class=&#34;m-destination-filter__form&#34;]/ul/li[2]/label/div&#39;).click()
#
# # Finding destination number on website
# driver.implicitly_wait(15)
# destinations = driver.find_element(by=By.XPATH, value=&#39;//span[@class=&#34;m-destination-list__count-label fs-copy&#34;]&#39;).text
#
# # Filter out destination number
# print(destinations)
# dest_num = &#34;&#34;
# for c in destinations:
#     if c.isdigit():
#         dest_num = dest_num + c
# print(&#34;Extracted numbers from the list : &#34; + dest_num)
#
# #TO DO remove for TEST
# dest_num = 1
#
# # Click on &#34;show more button&#34; to open up all destination subpages
# for i in range(1,int(dest_num)):
#     try:
#         driver.find_element(by=By.XPATH, value=&#39;//button[@class=&#34;m-teaser-list__more c-button c-button--secondary&#34;]&#39;).click()
#         driver.implicitly_wait(1)
#     except:
#         break
#     print(&#34;i: &#34;, i)
#
# ##########################################################################
# # Get tour_page link attributes from all huts and mountain_hike pages and save it a tour_list:
#
# # Collect all webelements of the huts &amp; hikes subpages:
# huts_hikes = driver.find_elements(by=By.XPATH, value=&#39;.//a[@class=&#34;c-teaser-destination__link&#34;]&#39;) #option:by Xpath
# #huts_hikes = driver.find_elements(by=By.CLASS_NAME, value=&#34;c-teaser-destination__link&#34;) #option:by class
#
# huts_hikes_link_list=[]
# tour_link_list=[]
#
# # Get links from selenium webelements huts &amp; hikes
# for item1 in huts_hikes:
#     href_huts_hikes = item1.get_attribute(&#39;href&#39;)
#     huts_hikes_link_list.append(href_huts_hikes)
#
# print(&#34;We have got &#34;, len(huts_hikes_link_list), &#34; huts and hike page links and visit now each of them.&#34;)
#
# # Open subpages and save tour_page links in the tour_link_list.
# # These tour pages will be our target pages to crawl information from:
# for item2 in huts_hikes_link_list:
#     driver.get(item2) # Open tour pages
#     tour_page = driver.find_elements(by=By.XPATH, value=&#39;.//*[@id=&#34;poi&#34;]/div/nav[1]/table/tbody/tr/td[1]/a&#39;)
#     for tp in tour_page:
#         try: # Extract tour_page links from webelement
#             href_tour_page = tp.get_attribute(&#39;href&#39;)
#             if href_tour_page not in tour_link_list: # Check of duplicates / only saving unique tour links
#                 tour_link_list.append(href_tour_page)
#             else:
#                 continue
#         except:
#             print(&#34;item3: no href attribute&#34;) # Releasing error, in case no tour page on the huts &amp; hikes page
#
# # Summary of our link list &amp; number of target pages:
# print(&#34;We have crawled &#34;,len(tour_link_list),&#34; tour_page links:&#34; )
# #print(tour_link_list)
#
# # Save the tour_link_list to a separate csv file
# # (useful in case of we have error messages in crawilng the data in the next step, we can run through this csv):
# link_data =[]
# for link in tour_link_list:
#     link_dict = {
#         &#39;link&#39;: link}
#     link_data.append(link_dict)
#
# df_tour_page_links = pd.DataFrame(link_data)
# print(df_tour_page_links.head(10))
#
# # Export the DataFrame to a CSV file
# df_tour_page_links.to_csv(&#34;SAC_page_links.csv&#34;,sep=&#39;;&#39;)
#
# print(&#34;Now we will crawl the information from all of them.&#34;)
#
# ##############################################################################
# #Tour_list contains all tour subsite links, which we are crawling below:
# tour_data=[]
# i=0
# for tour_page in tour_link_list:
#     driver.get(tour_page)
#     # Header from which we generate the title and subtitle data:
#     try:
#         header = driver.find_element(by=By.XPATH, value=&#39;/html/body/div[4]/div[5]/div[1]/div[2]/h2&#39;).text
#         title = header.split(&#34;\n&#34;)[0]
#         subtitle = header.split(&#34;\n&#34;)[1]
#     except:
#         header =&#34;na&#34;
#         title = &#34;na&#34;
#         subtitle = &#34;na&#34;
#     # Difficulty level in scale of T1 - T6:
#     try:
#         difficulty = driver.find_element(by=By.XPATH, value=&#39;.//*[@id=&#34;route&#34;]/div[3]/ul/li[1]/dl/dd/a&#39;).text
#     except:
#         difficulty = &#34;na&#34;
#     # Ascent, descent and time we do not have at all pages - we fill missing values with na:
#     try:
#         route_info_a = driver.find_element(by=By.XPATH, value=&#39;.//*[@id=&#34;route&#34;]/div[3]/ul/li[2]/dl/dd&#39;).text
#         time_a = route_info_a.split(&#34;,&#34;)[0]
#         ascent = route_info_a.split(&#34;,&#34;)[1]
#     except:
#         route_info_a = &#34;na&#34;
#         time_a = &#34;na&#34;
#         ascent = &#34;na&#34;
#     try:
#         route_info_d = driver.find_element(by=By.XPATH, value=&#39;.//*[@id=&#34;route&#34;]/div[3]/ul/li[3]/dl/dd&#39;).text
#         time_d = route_info_d.split(&#34;,&#34;)[0]
#         descent = route_info_d.split(&#34;,&#34;)[1]
#     except:
#         route_info_d = &#34;na&#34;
#         time_d = &#34;na&#34;
#         descent =&#34;na&#34;
#     # Map link which is unique for each tour page:
#     try:
#         map = driver.find_elements(by=By.XPATH, value=&#39;.//*[@hreflang=&#34;x-default&#34;]&#39;)
#         map_link =&#34;&#34;
#         for mp in map:
#             map_link = mp.get_attribute(&#39;href&#39;)
#     except:
#         map =&#34;na&#34;
#     # Description is located in an accordion in html, has different number of rows at each subpage
#     # we extract only first row of description with CSS selector, we do not extract the different variants of the tours:
#     try:
#         description = driver.find_element(By.CSS_SELECTOR,&#39;*&gt;div.m-route-accordion__description.c-rich-text&gt;p&#39;).text
#     except:
#         description = &#34;na&#34;
#     tour_dict_item = {
#         &#39;title&#39;: title,
#         &#39;subtitle&#39;: subtitle,
#         &#39;difficulty&#39;: difficulty,
#         &#39;time_ascent&#39;: time_a,
#         &#39;ascent&#39;: ascent,
#         &#39;time_descent&#39;: time_d,
#         &#39;descent&#39;: descent,
#         &#39;link&#39;: tour_page,
#         &#39;map&#39;: map_link,
#         &#39;description&#39;: description}
#     tour_data.append(tour_dict_item)
#     if i%10 == 0:
#         print(&#34;Get infos of &#34;, i ,&#34;/&#34;, len(tour_link_list), &#34; hiking tours.&#34;)
#     i+=1
#
# ###########################################################################
# # Creating and printing a data frame
# df = pd.DataFrame(tour_data)
# print(df.head(10))
#
# # Export the DataFrame to a CSV file, we create 2 files:
# # 1. saves the data in the same file / overwrites database (indexing is included)
# df.to_csv(&#34;SAC_data_with_index1.csv&#34;,sep=&#39;;&#39;)
#
# # 2. saves the data in the same file / overwrites database (indexing is disabled) -&gt; we can append it later
# df.to_csv(&#34;SAC_data_without_index1.csv&#34;,sep=&#39;;&#39;,index = False)
#
# # 3. appends an existing csv file with new data:
# #df.to_csv(&#34;SAC_data_without_index.cs&#34;,sep=&#39;;&#39;, mode =&#34;a&#34;, header = False, index = False)
#
# # Closing browser
# driver.quit()    # the selenium-controlled chrome browser is terminated
# #driver.close()    # the you don&#39;t see the result
#
# print(&#34;end SacExtractor.py&#34;)
# print(&#34;end&#34;)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="extractors" href="index.html">extractors</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>